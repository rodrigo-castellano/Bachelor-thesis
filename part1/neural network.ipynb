{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>COSMIC RADIATION: deep learning</b>\n",
    "    \n",
    "rodrigoX: Contains 4 parameters\n",
    "    1. NALLParticlesTotal : Total number of particles generated by the event in the ground level.\n",
    "    2. MUTotal : Total number of muons.\n",
    "    3. ELTotal : Total number of electromagnetic particles.\n",
    "    4. Zenith : Zenith angle of the particle [degrees].\n",
    "    5. Energy : Particle energy [GeV].\n",
    "\n",
    "rodrigoY: contains the target\n",
    "    Labels: photon, proton, helium, nitrogen, iron.\n",
    "        the smallest number is assigned to photon (A=0) and the highest number to the heaviest particle, this is, iron (B=4).\n",
    "        \n",
    "        0==photon     1==proton    2==helium     3==nitrogen   4==iron \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Dropout\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load it back:\n",
    "model = tf.keras.models.load_model('cosmic_radiation.model')\n",
    "#to save the model\n",
    "#model.save('cosmic_radiation.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#leave the dataset ready\n",
    "df_x = pd.read_fwf('XRodrigo.txt')\n",
    "df_x.columns = [\"NALLParticlesTotal\", \"MUTotal\", \"ELTotal\", \"Zenith\",\"Energy\"]\n",
    "df_y=pd.read_fwf('YRodrigo.txt')\n",
    "df_y.columns = [\"Particle\"]\n",
    "\n",
    "transpose=df_x.T\n",
    "transpose_y=df_y.T #Transpose to put them together\n",
    "df_tot = transpose.append(transpose_y)\n",
    "df_tot=df_tot.T.sample(frac=1).reset_index(drop=True)#este el el df completo y barajado\n",
    "\n",
    "df_y=df_tot[['Particle']]\n",
    "df_x=df_tot[[\"NALLParticlesTotal\", \"MUTotal\", \"ELTotal\", \"Zenith\",\"Energy\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train-test split and reescale\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_x, df_y, random_state=0,test_size = 0.25)\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "y_train_np=y_train.to_numpy()\n",
    "y_test_np=y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization\n",
    "# from tensorflow.keras.layers import Dropout\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.Dense(128, use_bias=False,kernel_initializer='he_uniform', activation=tf.nn.relu))\n",
    "model.add(BatchNormalization())\n",
    "model.add(tf.keras.layers.Dropout(0.0))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(128, use_bias=False,kernel_initializer='he_uniform', activation=tf.nn.relu))\n",
    "model.add(BatchNormalization())\n",
    "model.add(tf.keras.layers.Dropout(0.0))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(128, use_bias=False,kernel_initializer='he_uniform', activation=tf.nn.relu))\n",
    "model.add(BatchNormalization())\n",
    "model.add(tf.keras.layers.Dropout(0.0))\n",
    "\n",
    "\n",
    "\n",
    "model.add(tf.keras.layers.Dense(5, activation=tf.nn.softmax))\n",
    "\n",
    "# N_TRAIN=X_train.shape[0];BATCH_SIZE=128\n",
    "# STEPS_PER_EPOCH = N_TRAIN//BATCH_SIZE\n",
    "# lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay( 0.001,decay_steps=STEPS_PER_EPOCH*500,decay_rate=1,staircase=False)\n",
    "# #STEPS_PER_EPOCH*300-->The code above sets a schedules.InverseTimeDecay to hyperbolically decrease the learning rate to 1/2 of the base rate at 300 epochs, 1/3 at 600 epochs and so on.\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "\n",
    "model.compile(optimizer=optimizer , loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "#If I want to know the weights of a layer:\n",
    "#model.layers[1].get_weights()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "history=model.fit(X_train, y_train_np, epochs=500, batch_size=128, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_acc = model.evaluate(X_test, y_test_np)\n",
    "# print(val_loss)\n",
    "print(val_acc)\n",
    "#model.summary()\n",
    "#model.get_config()\n",
    "#model.get_weights()\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "y_pred= model.predict_classes(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(accuracy_score(y_test, datatest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "history1=history\n",
    "# plot learning curves\n",
    "pyplot.title('Learning Curves, ADAM')\n",
    "pyplot.xlabel('Epoch')\n",
    "pyplot.ylabel('Cross Entropy')\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "#pyplot.plot(history.history['val_loss'], label='val')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shouls chage\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import BatchNormalization\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(neurons=128,optimizer='adam',learn_rate=0.001, init_mode='he_normal', activation='relu', dropout_rate=0.0, weight_constraint=0):#parameters here are default\n",
    "    #create model\n",
    "    model = tf.keras.models.Sequential()\n",
    "    #los weight_initializer son mas optimos dependiendo de la funcion de activacion\n",
    "    model.add(tf.keras.layers.Dense(neurons, use_bias=False, kernel_initializer=init_mode, activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(neurons, use_bias=False, kernel_initializer=init_mode, activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(5, activation='softmax'))\n",
    "    \n",
    "    #aqui estoy obligando a que el optimizer sea adam. Deberia decir: if optimizer=='adam' then... y hacerlo para otros optimizadores\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learn_rate, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "    #compile model        https://keras.io/models/model/\n",
    "    model.compile(optimizer=optimizer,loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "model = KerasClassifier(build_fn=create_model, epochs=25, batch_size=64, verbose=1) #if I add any param here has to be in the model(there has to be a default value)#de values that I set here are definitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS  SI QUIERO CUALQUIER PARAMETRO QUE NO SEA EL DEFAULT LO PONGO AQUI ABAJO\n",
    "batch_size = [128,264]\n",
    "epochs = [5]\n",
    "optimizer = ['Adam','Nadam','Adagrad']#,'SGD', 'RMSprop', 'Adagrad', 'Adadelta',  'Adamax', 'Nadam']\n",
    "learn_rate = [0.001,0.01,0.05]#[0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "momentum = [0]#[0.0, 0.2, 0.4, 0.6, 0.8, 0.9]  #en este caso momentum es para SGD por ej., para adam solo se usa learn. rate\n",
    "activation = ['relu']#['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
    "init_mode = ['he_normal','he_uniform']#['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\n",
    "weight_constraint = [0]#[1, 2, 3, 4, 5]\n",
    "dropout_rate =[0] #[0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "neurons = [32,64,128,256] #[5, 10, 15, 20, 25, 30]\n",
    "\n",
    "param_grid = dict(epochs=epochs,batch_size=batch_size,optimizer=optimizer, init_mode=init_mode, neurons=neurons )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   41.9s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 240 out of 240 | elapsed:  4.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "31493/31493 [==============================] - 1s 17us/step - loss: 1.4484 - accuracy: 0.3674\n",
      "Epoch 2/5\n",
      "31493/31493 [==============================] - 0s 14us/step - loss: 1.0888 - accuracy: 0.5987\n",
      "Epoch 3/5\n",
      "31493/31493 [==============================] - 0s 13us/step - loss: 0.9006 - accuracy: 0.6985\n",
      "Epoch 4/5\n",
      "31493/31493 [==============================] - 0s 12us/step - loss: 0.7923 - accuracy: 0.7461\n",
      "Epoch 5/5\n",
      "31493/31493 [==============================] - 0s 12us/step - loss: 0.7258 - accuracy: 0.7671\n"
     ]
    }
   ],
   "source": [
    "#FIT GRID SEARCH\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=5, verbose=1)\n",
    "#FIT RANDOM GRID SEARCH\n",
    "#grid = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_jobs=-1, cv=2)\n",
    "grid_result = grid.fit(X_train, y_train_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.802337 using {'batch_size': 128, 'epochs': 5, 'init_mode': 'he_uniform', 'neurons': 32, 'optimizer': 'Nadam'}\n"
     ]
    }
   ],
   "source": [
    "# summarize results\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "# means = grid_result.cv_results_['mean_test_score']\n",
    "# stds = grid_result.cv_results_['std_test_score']\n",
    "# params = grid_result.cv_results_['params']\n",
    "# for mean, stdev, param in zip(means, stds, params):\n",
    "#     print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(grid_result.cv_results_)\n",
    "results.to_csv('random-grid-search-results-01.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_batch_size</th>\n",
       "      <th>param_epochs</th>\n",
       "      <th>param_init_mode</th>\n",
       "      <th>param_neurons</th>\n",
       "      <th>param_optimizer</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.391154</td>\n",
       "      <td>0.096141</td>\n",
       "      <td>0.384372</td>\n",
       "      <td>0.040880</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>he_normal</td>\n",
       "      <td>32</td>\n",
       "      <td>Adam</td>\n",
       "      <td>{'batch_size': 128, 'epochs': 5, 'init_mode': ...</td>\n",
       "      <td>0.700270</td>\n",
       "      <td>0.679790</td>\n",
       "      <td>0.758057</td>\n",
       "      <td>0.630676</td>\n",
       "      <td>0.689584</td>\n",
       "      <td>0.691676</td>\n",
       "      <td>0.040858</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.659888</td>\n",
       "      <td>0.099530</td>\n",
       "      <td>0.179922</td>\n",
       "      <td>0.044196</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>he_normal</td>\n",
       "      <td>32</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>{'batch_size': 128, 'epochs': 5, 'init_mode': ...</td>\n",
       "      <td>0.779171</td>\n",
       "      <td>0.787109</td>\n",
       "      <td>0.741705</td>\n",
       "      <td>0.790092</td>\n",
       "      <td>0.748809</td>\n",
       "      <td>0.769377</td>\n",
       "      <td>0.020141</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.781811</td>\n",
       "      <td>0.043954</td>\n",
       "      <td>0.217328</td>\n",
       "      <td>0.051169</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>he_normal</td>\n",
       "      <td>32</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>{'batch_size': 128, 'epochs': 5, 'init_mode': ...</td>\n",
       "      <td>0.628195</td>\n",
       "      <td>0.598190</td>\n",
       "      <td>0.632640</td>\n",
       "      <td>0.695300</td>\n",
       "      <td>0.583042</td>\n",
       "      <td>0.627474</td>\n",
       "      <td>0.038625</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.620494</td>\n",
       "      <td>0.193933</td>\n",
       "      <td>0.200297</td>\n",
       "      <td>0.042915</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>he_normal</td>\n",
       "      <td>64</td>\n",
       "      <td>Adam</td>\n",
       "      <td>{'batch_size': 128, 'epochs': 5, 'init_mode': ...</td>\n",
       "      <td>0.702651</td>\n",
       "      <td>0.761867</td>\n",
       "      <td>0.749960</td>\n",
       "      <td>0.787234</td>\n",
       "      <td>0.753255</td>\n",
       "      <td>0.750994</td>\n",
       "      <td>0.027476</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.996305</td>\n",
       "      <td>0.298421</td>\n",
       "      <td>0.207844</td>\n",
       "      <td>0.047457</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>he_normal</td>\n",
       "      <td>64</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>{'batch_size': 128, 'epochs': 5, 'init_mode': ...</td>\n",
       "      <td>0.789967</td>\n",
       "      <td>0.800286</td>\n",
       "      <td>0.837276</td>\n",
       "      <td>0.766751</td>\n",
       "      <td>0.787710</td>\n",
       "      <td>0.796398</td>\n",
       "      <td>0.023160</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.367496</td>\n",
       "      <td>0.380685</td>\n",
       "      <td>0.226120</td>\n",
       "      <td>0.056022</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>he_normal</td>\n",
       "      <td>64</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>{'batch_size': 128, 'epochs': 5, 'init_mode': ...</td>\n",
       "      <td>0.753453</td>\n",
       "      <td>0.700270</td>\n",
       "      <td>0.729798</td>\n",
       "      <td>0.741188</td>\n",
       "      <td>0.725786</td>\n",
       "      <td>0.730099</td>\n",
       "      <td>0.017759</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.437501</td>\n",
       "      <td>0.275093</td>\n",
       "      <td>0.281184</td>\n",
       "      <td>0.058450</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>he_normal</td>\n",
       "      <td>128</td>\n",
       "      <td>Adam</td>\n",
       "      <td>{'batch_size': 128, 'epochs': 5, 'init_mode': ...</td>\n",
       "      <td>0.722496</td>\n",
       "      <td>0.848071</td>\n",
       "      <td>0.736149</td>\n",
       "      <td>0.741505</td>\n",
       "      <td>0.825341</td>\n",
       "      <td>0.774712</td>\n",
       "      <td>0.051500</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4.317676</td>\n",
       "      <td>0.416782</td>\n",
       "      <td>0.339747</td>\n",
       "      <td>0.096444</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>he_normal</td>\n",
       "      <td>128</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>{'batch_size': 128, 'epochs': 5, 'init_mode': ...</td>\n",
       "      <td>0.841403</td>\n",
       "      <td>0.800445</td>\n",
       "      <td>0.751230</td>\n",
       "      <td>0.848841</td>\n",
       "      <td>0.729120</td>\n",
       "      <td>0.794208</td>\n",
       "      <td>0.047612</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.829948</td>\n",
       "      <td>0.295035</td>\n",
       "      <td>0.357485</td>\n",
       "      <td>0.121151</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>he_normal</td>\n",
       "      <td>128</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>{'batch_size': 128, 'epochs': 5, 'init_mode': ...</td>\n",
       "      <td>0.722019</td>\n",
       "      <td>0.739165</td>\n",
       "      <td>0.772345</td>\n",
       "      <td>0.783582</td>\n",
       "      <td>0.725945</td>\n",
       "      <td>0.748611</td>\n",
       "      <td>0.024885</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6.351973</td>\n",
       "      <td>0.540787</td>\n",
       "      <td>0.470245</td>\n",
       "      <td>0.143196</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>he_normal</td>\n",
       "      <td>256</td>\n",
       "      <td>Adam</td>\n",
       "      <td>{'batch_size': 128, 'epochs': 5, 'init_mode': ...</td>\n",
       "      <td>0.774726</td>\n",
       "      <td>0.777743</td>\n",
       "      <td>0.852040</td>\n",
       "      <td>0.723563</td>\n",
       "      <td>0.763417</td>\n",
       "      <td>0.778298</td>\n",
       "      <td>0.041637</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>7.665792</td>\n",
       "      <td>0.610991</td>\n",
       "      <td>0.496756</td>\n",
       "      <td>0.164851</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>he_normal</td>\n",
       "      <td>256</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>{'batch_size': 128, 'epochs': 5, 'init_mode': ...</td>\n",
       "      <td>0.782981</td>\n",
       "      <td>0.714399</td>\n",
       "      <td>0.673758</td>\n",
       "      <td>0.795967</td>\n",
       "      <td>0.690378</td>\n",
       "      <td>0.731497</td>\n",
       "      <td>0.049242</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6.020743</td>\n",
       "      <td>0.076090</td>\n",
       "      <td>0.406754</td>\n",
       "      <td>0.138311</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>he_normal</td>\n",
       "      <td>256</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>{'batch_size': 128, 'epochs': 5, 'init_mode': ...</td>\n",
       "      <td>0.774885</td>\n",
       "      <td>0.738212</td>\n",
       "      <td>0.767741</td>\n",
       "      <td>0.768498</td>\n",
       "      <td>0.824547</td>\n",
       "      <td>0.774777</td>\n",
       "      <td>0.027939</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3.570344</td>\n",
       "      <td>1.114939</td>\n",
       "      <td>0.211549</td>\n",
       "      <td>0.020979</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>he_uniform</td>\n",
       "      <td>32</td>\n",
       "      <td>Adam</td>\n",
       "      <td>{'batch_size': 128, 'epochs': 5, 'init_mode': ...</td>\n",
       "      <td>0.745039</td>\n",
       "      <td>0.670424</td>\n",
       "      <td>0.713605</td>\n",
       "      <td>0.707844</td>\n",
       "      <td>0.637663</td>\n",
       "      <td>0.694915</td>\n",
       "      <td>0.037166</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3.086701</td>\n",
       "      <td>0.152967</td>\n",
       "      <td>0.207146</td>\n",
       "      <td>0.039035</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>he_uniform</td>\n",
       "      <td>32</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>{'batch_size': 128, 'epochs': 5, 'init_mode': ...</td>\n",
       "      <td>0.822670</td>\n",
       "      <td>0.768852</td>\n",
       "      <td>0.822670</td>\n",
       "      <td>0.806288</td>\n",
       "      <td>0.791204</td>\n",
       "      <td>0.802337</td>\n",
       "      <td>0.020434</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2.239944</td>\n",
       "      <td>0.178535</td>\n",
       "      <td>0.215869</td>\n",
       "      <td>0.016030</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>he_uniform</td>\n",
       "      <td>32</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>{'batch_size': 128, 'epochs': 5, 'init_mode': ...</td>\n",
       "      <td>0.639149</td>\n",
       "      <td>0.629306</td>\n",
       "      <td>0.623750</td>\n",
       "      <td>0.673706</td>\n",
       "      <td>0.603525</td>\n",
       "      <td>0.633887</td>\n",
       "      <td>0.023060</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3.120496</td>\n",
       "      <td>0.338670</td>\n",
       "      <td>0.231616</td>\n",
       "      <td>0.033727</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>he_uniform</td>\n",
       "      <td>64</td>\n",
       "      <td>Adam</td>\n",
       "      <td>{'batch_size': 128, 'epochs': 5, 'init_mode': ...</td>\n",
       "      <td>0.795523</td>\n",
       "      <td>0.738054</td>\n",
       "      <td>0.713764</td>\n",
       "      <td>0.807082</td>\n",
       "      <td>0.815815</td>\n",
       "      <td>0.774047</td>\n",
       "      <td>0.040563</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3.340493</td>\n",
       "      <td>0.160609</td>\n",
       "      <td>0.280253</td>\n",
       "      <td>0.102356</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>he_uniform</td>\n",
       "      <td>64</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>{'batch_size': 128, 'epochs': 5, 'init_mode': ...</td>\n",
       "      <td>0.807430</td>\n",
       "      <td>0.738847</td>\n",
       "      <td>0.707573</td>\n",
       "      <td>0.772467</td>\n",
       "      <td>0.791045</td>\n",
       "      <td>0.763472</td>\n",
       "      <td>0.036068</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2.397735</td>\n",
       "      <td>0.284636</td>\n",
       "      <td>0.286100</td>\n",
       "      <td>0.114380</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>he_uniform</td>\n",
       "      <td>64</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>{'batch_size': 128, 'epochs': 5, 'init_mode': ...</td>\n",
       "      <td>0.668519</td>\n",
       "      <td>0.686776</td>\n",
       "      <td>0.701699</td>\n",
       "      <td>0.766116</td>\n",
       "      <td>0.737059</td>\n",
       "      <td>0.712034</td>\n",
       "      <td>0.035184</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3.915668</td>\n",
       "      <td>0.441586</td>\n",
       "      <td>0.232191</td>\n",
       "      <td>0.068015</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>he_uniform</td>\n",
       "      <td>128</td>\n",
       "      <td>Adam</td>\n",
       "      <td>{'batch_size': 128, 'epochs': 5, 'init_mode': ...</td>\n",
       "      <td>0.773297</td>\n",
       "      <td>0.757422</td>\n",
       "      <td>0.817114</td>\n",
       "      <td>0.796443</td>\n",
       "      <td>0.776913</td>\n",
       "      <td>0.784238</td>\n",
       "      <td>0.020602</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4.797312</td>\n",
       "      <td>0.149895</td>\n",
       "      <td>0.320819</td>\n",
       "      <td>0.176451</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>he_uniform</td>\n",
       "      <td>128</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>{'batch_size': 128, 'epochs': 5, 'init_mode': ...</td>\n",
       "      <td>0.793301</td>\n",
       "      <td>0.801238</td>\n",
       "      <td>0.827909</td>\n",
       "      <td>0.721340</td>\n",
       "      <td>0.768974</td>\n",
       "      <td>0.782552</td>\n",
       "      <td>0.035926</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3.257850</td>\n",
       "      <td>0.177980</td>\n",
       "      <td>0.324824</td>\n",
       "      <td>0.059822</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>he_uniform</td>\n",
       "      <td>128</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>{'batch_size': 128, 'epochs': 5, 'init_mode': ...</td>\n",
       "      <td>0.753135</td>\n",
       "      <td>0.614224</td>\n",
       "      <td>0.751072</td>\n",
       "      <td>0.678628</td>\n",
       "      <td>0.738965</td>\n",
       "      <td>0.707205</td>\n",
       "      <td>0.053860</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>6.746908</td>\n",
       "      <td>0.868581</td>\n",
       "      <td>0.482052</td>\n",
       "      <td>0.173311</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>he_uniform</td>\n",
       "      <td>256</td>\n",
       "      <td>Adam</td>\n",
       "      <td>{'batch_size': 128, 'epochs': 5, 'init_mode': ...</td>\n",
       "      <td>0.822988</td>\n",
       "      <td>0.785204</td>\n",
       "      <td>0.763137</td>\n",
       "      <td>0.751191</td>\n",
       "      <td>0.786758</td>\n",
       "      <td>0.781855</td>\n",
       "      <td>0.024568</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>7.537841</td>\n",
       "      <td>0.589888</td>\n",
       "      <td>0.463529</td>\n",
       "      <td>0.112301</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>he_uniform</td>\n",
       "      <td>256</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>{'batch_size': 128, 'epochs': 5, 'init_mode': ...</td>\n",
       "      <td>0.645023</td>\n",
       "      <td>0.739324</td>\n",
       "      <td>0.733132</td>\n",
       "      <td>0.655923</td>\n",
       "      <td>0.835345</td>\n",
       "      <td>0.721749</td>\n",
       "      <td>0.068647</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>6.238195</td>\n",
       "      <td>0.765762</td>\n",
       "      <td>0.441028</td>\n",
       "      <td>0.154699</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>he_uniform</td>\n",
       "      <td>256</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>{'batch_size': 128, 'epochs': 5, 'init_mode': ...</td>\n",
       "      <td>0.695348</td>\n",
       "      <td>0.805048</td>\n",
       "      <td>0.773139</td>\n",
       "      <td>0.639886</td>\n",
       "      <td>0.742617</td>\n",
       "      <td>0.731208</td>\n",
       "      <td>0.058252</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2.324199</td>\n",
       "      <td>0.435338</td>\n",
       "      <td>0.188184</td>\n",
       "      <td>0.032121</td>\n",
       "      <td>264</td>\n",
       "      <td>5</td>\n",
       "      <td>he_normal</td>\n",
       "      <td>32</td>\n",
       "      <td>Adam</td>\n",
       "      <td>{'batch_size': 264, 'epochs': 5, 'init_mode': ...</td>\n",
       "      <td>0.632481</td>\n",
       "      <td>0.607557</td>\n",
       "      <td>0.562311</td>\n",
       "      <td>0.554621</td>\n",
       "      <td>0.611464</td>\n",
       "      <td>0.593687</td>\n",
       "      <td>0.030080</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2.214063</td>\n",
       "      <td>0.057130</td>\n",
       "      <td>0.186553</td>\n",
       "      <td>0.036686</td>\n",
       "      <td>264</td>\n",
       "      <td>5</td>\n",
       "      <td>he_normal</td>\n",
       "      <td>32</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>{'batch_size': 264, 'epochs': 5, 'init_mode': ...</td>\n",
       "      <td>0.659311</td>\n",
       "      <td>0.729005</td>\n",
       "      <td>0.726147</td>\n",
       "      <td>0.659892</td>\n",
       "      <td>0.673230</td>\n",
       "      <td>0.689517</td>\n",
       "      <td>0.031484</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.799853</td>\n",
       "      <td>0.257190</td>\n",
       "      <td>0.205163</td>\n",
       "      <td>0.051080</td>\n",
       "      <td>264</td>\n",
       "      <td>5</td>\n",
       "      <td>he_normal</td>\n",
       "      <td>32</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>{'batch_size': 264, 'epochs': 5, 'init_mode': ...</td>\n",
       "      <td>0.596920</td>\n",
       "      <td>0.604858</td>\n",
       "      <td>0.517542</td>\n",
       "      <td>0.570181</td>\n",
       "      <td>0.574468</td>\n",
       "      <td>0.572794</td>\n",
       "      <td>0.030570</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2.277235</td>\n",
       "      <td>0.112759</td>\n",
       "      <td>0.205107</td>\n",
       "      <td>0.004444</td>\n",
       "      <td>264</td>\n",
       "      <td>5</td>\n",
       "      <td>he_normal</td>\n",
       "      <td>64</td>\n",
       "      <td>Adam</td>\n",
       "      <td>{'batch_size': 264, 'epochs': 5, 'init_mode': ...</td>\n",
       "      <td>0.691697</td>\n",
       "      <td>0.723131</td>\n",
       "      <td>0.694396</td>\n",
       "      <td>0.690219</td>\n",
       "      <td>0.691331</td>\n",
       "      <td>0.698155</td>\n",
       "      <td>0.012563</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2.668551</td>\n",
       "      <td>0.086737</td>\n",
       "      <td>0.232035</td>\n",
       "      <td>0.055312</td>\n",
       "      <td>264</td>\n",
       "      <td>5</td>\n",
       "      <td>he_normal</td>\n",
       "      <td>64</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>{'batch_size': 264, 'epochs': 5, 'init_mode': ...</td>\n",
       "      <td>0.792507</td>\n",
       "      <td>0.741070</td>\n",
       "      <td>0.705826</td>\n",
       "      <td>0.616704</td>\n",
       "      <td>0.781677</td>\n",
       "      <td>0.727557</td>\n",
       "      <td>0.063389</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.978885</td>\n",
       "      <td>0.120054</td>\n",
       "      <td>0.232963</td>\n",
       "      <td>0.078864</td>\n",
       "      <td>264</td>\n",
       "      <td>5</td>\n",
       "      <td>he_normal</td>\n",
       "      <td>64</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>{'batch_size': 264, 'epochs': 5, 'init_mode': ...</td>\n",
       "      <td>0.730751</td>\n",
       "      <td>0.689316</td>\n",
       "      <td>0.645658</td>\n",
       "      <td>0.686726</td>\n",
       "      <td>0.642585</td>\n",
       "      <td>0.679007</td>\n",
       "      <td>0.032502</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2.789785</td>\n",
       "      <td>0.280597</td>\n",
       "      <td>0.260028</td>\n",
       "      <td>0.085120</td>\n",
       "      <td>264</td>\n",
       "      <td>5</td>\n",
       "      <td>he_normal</td>\n",
       "      <td>128</td>\n",
       "      <td>Adam</td>\n",
       "      <td>{'batch_size': 264, 'epochs': 5, 'init_mode': ...</td>\n",
       "      <td>0.752500</td>\n",
       "      <td>0.743293</td>\n",
       "      <td>0.722178</td>\n",
       "      <td>0.745316</td>\n",
       "      <td>0.737377</td>\n",
       "      <td>0.740133</td>\n",
       "      <td>0.010196</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>3.483399</td>\n",
       "      <td>0.281094</td>\n",
       "      <td>0.284199</td>\n",
       "      <td>0.073868</td>\n",
       "      <td>264</td>\n",
       "      <td>5</td>\n",
       "      <td>he_normal</td>\n",
       "      <td>128</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>{'batch_size': 264, 'epochs': 5, 'init_mode': ...</td>\n",
       "      <td>0.776314</td>\n",
       "      <td>0.548024</td>\n",
       "      <td>0.742499</td>\n",
       "      <td>0.802477</td>\n",
       "      <td>0.745792</td>\n",
       "      <td>0.723021</td>\n",
       "      <td>0.090195</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2.483238</td>\n",
       "      <td>0.172223</td>\n",
       "      <td>0.237828</td>\n",
       "      <td>0.097553</td>\n",
       "      <td>264</td>\n",
       "      <td>5</td>\n",
       "      <td>he_normal</td>\n",
       "      <td>128</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>{'batch_size': 264, 'epochs': 5, 'init_mode': ...</td>\n",
       "      <td>0.624861</td>\n",
       "      <td>0.754564</td>\n",
       "      <td>0.703445</td>\n",
       "      <td>0.710384</td>\n",
       "      <td>0.635440</td>\n",
       "      <td>0.685739</td>\n",
       "      <td>0.048773</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>4.680945</td>\n",
       "      <td>0.566995</td>\n",
       "      <td>0.528591</td>\n",
       "      <td>0.297240</td>\n",
       "      <td>264</td>\n",
       "      <td>5</td>\n",
       "      <td>he_normal</td>\n",
       "      <td>256</td>\n",
       "      <td>Adam</td>\n",
       "      <td>{'batch_size': 264, 'epochs': 5, 'init_mode': ...</td>\n",
       "      <td>0.749484</td>\n",
       "      <td>0.699476</td>\n",
       "      <td>0.770281</td>\n",
       "      <td>0.759130</td>\n",
       "      <td>0.730391</td>\n",
       "      <td>0.741752</td>\n",
       "      <td>0.024868</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>5.733126</td>\n",
       "      <td>0.306495</td>\n",
       "      <td>0.469986</td>\n",
       "      <td>0.379053</td>\n",
       "      <td>264</td>\n",
       "      <td>5</td>\n",
       "      <td>he_normal</td>\n",
       "      <td>256</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>{'batch_size': 264, 'epochs': 5, 'init_mode': ...</td>\n",
       "      <td>0.703286</td>\n",
       "      <td>0.794729</td>\n",
       "      <td>0.735990</td>\n",
       "      <td>0.643220</td>\n",
       "      <td>0.752461</td>\n",
       "      <td>0.725937</td>\n",
       "      <td>0.050778</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>3.948307</td>\n",
       "      <td>0.397142</td>\n",
       "      <td>0.406174</td>\n",
       "      <td>0.174878</td>\n",
       "      <td>264</td>\n",
       "      <td>5</td>\n",
       "      <td>he_normal</td>\n",
       "      <td>256</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>{'batch_size': 264, 'epochs': 5, 'init_mode': ...</td>\n",
       "      <td>0.735831</td>\n",
       "      <td>0.699159</td>\n",
       "      <td>0.646293</td>\n",
       "      <td>0.702922</td>\n",
       "      <td>0.747380</td>\n",
       "      <td>0.706317</td>\n",
       "      <td>0.035279</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2.662696</td>\n",
       "      <td>0.937792</td>\n",
       "      <td>0.199899</td>\n",
       "      <td>0.041986</td>\n",
       "      <td>264</td>\n",
       "      <td>5</td>\n",
       "      <td>he_uniform</td>\n",
       "      <td>32</td>\n",
       "      <td>Adam</td>\n",
       "      <td>{'batch_size': 264, 'epochs': 5, 'init_mode': ...</td>\n",
       "      <td>0.628036</td>\n",
       "      <td>0.542943</td>\n",
       "      <td>0.505001</td>\n",
       "      <td>0.651318</td>\n",
       "      <td>0.645602</td>\n",
       "      <td>0.594580</td>\n",
       "      <td>0.059384</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2.236734</td>\n",
       "      <td>0.091245</td>\n",
       "      <td>0.189377</td>\n",
       "      <td>0.031969</td>\n",
       "      <td>264</td>\n",
       "      <td>5</td>\n",
       "      <td>he_uniform</td>\n",
       "      <td>32</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>{'batch_size': 264, 'epochs': 5, 'init_mode': ...</td>\n",
       "      <td>0.668678</td>\n",
       "      <td>0.752659</td>\n",
       "      <td>0.725512</td>\n",
       "      <td>0.674659</td>\n",
       "      <td>0.694983</td>\n",
       "      <td>0.703298</td>\n",
       "      <td>0.031679</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1.628574</td>\n",
       "      <td>0.082701</td>\n",
       "      <td>0.195515</td>\n",
       "      <td>0.030234</td>\n",
       "      <td>264</td>\n",
       "      <td>5</td>\n",
       "      <td>he_uniform</td>\n",
       "      <td>32</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>{'batch_size': 264, 'epochs': 5, 'init_mode': ...</td>\n",
       "      <td>0.607557</td>\n",
       "      <td>0.632958</td>\n",
       "      <td>0.546912</td>\n",
       "      <td>0.608447</td>\n",
       "      <td>0.624484</td>\n",
       "      <td>0.604072</td>\n",
       "      <td>0.030165</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2.174192</td>\n",
       "      <td>0.196103</td>\n",
       "      <td>0.243445</td>\n",
       "      <td>0.084013</td>\n",
       "      <td>264</td>\n",
       "      <td>5</td>\n",
       "      <td>he_uniform</td>\n",
       "      <td>64</td>\n",
       "      <td>Adam</td>\n",
       "      <td>{'batch_size': 264, 'epochs': 5, 'init_mode': ...</td>\n",
       "      <td>0.666296</td>\n",
       "      <td>0.751548</td>\n",
       "      <td>0.649786</td>\n",
       "      <td>0.683709</td>\n",
       "      <td>0.698158</td>\n",
       "      <td>0.689899</td>\n",
       "      <td>0.034852</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2.706820</td>\n",
       "      <td>0.130739</td>\n",
       "      <td>0.222837</td>\n",
       "      <td>0.072213</td>\n",
       "      <td>264</td>\n",
       "      <td>5</td>\n",
       "      <td>he_uniform</td>\n",
       "      <td>64</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>{'batch_size': 264, 'epochs': 5, 'init_mode': ...</td>\n",
       "      <td>0.665185</td>\n",
       "      <td>0.779013</td>\n",
       "      <td>0.742816</td>\n",
       "      <td>0.666720</td>\n",
       "      <td>0.646713</td>\n",
       "      <td>0.700089</td>\n",
       "      <td>0.051450</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2.228038</td>\n",
       "      <td>0.323113</td>\n",
       "      <td>0.255989</td>\n",
       "      <td>0.045270</td>\n",
       "      <td>264</td>\n",
       "      <td>5</td>\n",
       "      <td>he_uniform</td>\n",
       "      <td>64</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>{'batch_size': 264, 'epochs': 5, 'init_mode': ...</td>\n",
       "      <td>0.693920</td>\n",
       "      <td>0.589935</td>\n",
       "      <td>0.671218</td>\n",
       "      <td>0.538901</td>\n",
       "      <td>0.655129</td>\n",
       "      <td>0.629820</td>\n",
       "      <td>0.057132</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2.685289</td>\n",
       "      <td>0.222952</td>\n",
       "      <td>0.255982</td>\n",
       "      <td>0.090630</td>\n",
       "      <td>264</td>\n",
       "      <td>5</td>\n",
       "      <td>he_uniform</td>\n",
       "      <td>128</td>\n",
       "      <td>Adam</td>\n",
       "      <td>{'batch_size': 264, 'epochs': 5, 'init_mode': ...</td>\n",
       "      <td>0.733608</td>\n",
       "      <td>0.695983</td>\n",
       "      <td>0.773297</td>\n",
       "      <td>0.713560</td>\n",
       "      <td>0.755478</td>\n",
       "      <td>0.734385</td>\n",
       "      <td>0.027814</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>3.249573</td>\n",
       "      <td>0.116436</td>\n",
       "      <td>0.273327</td>\n",
       "      <td>0.116947</td>\n",
       "      <td>264</td>\n",
       "      <td>5</td>\n",
       "      <td>he_uniform</td>\n",
       "      <td>128</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>{'batch_size': 264, 'epochs': 5, 'init_mode': ...</td>\n",
       "      <td>0.631529</td>\n",
       "      <td>0.722813</td>\n",
       "      <td>0.701540</td>\n",
       "      <td>0.584789</td>\n",
       "      <td>0.734836</td>\n",
       "      <td>0.675101</td>\n",
       "      <td>0.057617</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2.501010</td>\n",
       "      <td>0.166296</td>\n",
       "      <td>0.379093</td>\n",
       "      <td>0.212618</td>\n",
       "      <td>264</td>\n",
       "      <td>5</td>\n",
       "      <td>he_uniform</td>\n",
       "      <td>128</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>{'batch_size': 264, 'epochs': 5, 'init_mode': ...</td>\n",
       "      <td>0.702810</td>\n",
       "      <td>0.573901</td>\n",
       "      <td>0.668360</td>\n",
       "      <td>0.608606</td>\n",
       "      <td>0.661162</td>\n",
       "      <td>0.642968</td>\n",
       "      <td>0.045843</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>4.848362</td>\n",
       "      <td>0.569429</td>\n",
       "      <td>0.416948</td>\n",
       "      <td>0.151584</td>\n",
       "      <td>264</td>\n",
       "      <td>5</td>\n",
       "      <td>he_uniform</td>\n",
       "      <td>256</td>\n",
       "      <td>Adam</td>\n",
       "      <td>{'batch_size': 264, 'epochs': 5, 'init_mode': ...</td>\n",
       "      <td>0.766788</td>\n",
       "      <td>0.763931</td>\n",
       "      <td>0.767423</td>\n",
       "      <td>0.785011</td>\n",
       "      <td>0.740076</td>\n",
       "      <td>0.764646</td>\n",
       "      <td>0.014361</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>5.712243</td>\n",
       "      <td>0.369797</td>\n",
       "      <td>0.467753</td>\n",
       "      <td>0.177920</td>\n",
       "      <td>264</td>\n",
       "      <td>5</td>\n",
       "      <td>he_uniform</td>\n",
       "      <td>256</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>{'batch_size': 264, 'epochs': 5, 'init_mode': ...</td>\n",
       "      <td>0.670424</td>\n",
       "      <td>0.754247</td>\n",
       "      <td>0.628036</td>\n",
       "      <td>0.687837</td>\n",
       "      <td>0.596539</td>\n",
       "      <td>0.667417</td>\n",
       "      <td>0.053926</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>5.250894</td>\n",
       "      <td>0.778133</td>\n",
       "      <td>0.520356</td>\n",
       "      <td>0.383205</td>\n",
       "      <td>264</td>\n",
       "      <td>5</td>\n",
       "      <td>he_uniform</td>\n",
       "      <td>256</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>{'batch_size': 264, 'epochs': 5, 'init_mode': ...</td>\n",
       "      <td>0.691221</td>\n",
       "      <td>0.688204</td>\n",
       "      <td>0.700587</td>\n",
       "      <td>0.608765</td>\n",
       "      <td>0.615751</td>\n",
       "      <td>0.660906</td>\n",
       "      <td>0.039991</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        2.391154      0.096141         0.384372        0.040880   \n",
       "1        2.659888      0.099530         0.179922        0.044196   \n",
       "2        1.781811      0.043954         0.217328        0.051169   \n",
       "3        2.620494      0.193933         0.200297        0.042915   \n",
       "4        2.996305      0.298421         0.207844        0.047457   \n",
       "5        2.367496      0.380685         0.226120        0.056022   \n",
       "6        3.437501      0.275093         0.281184        0.058450   \n",
       "7        4.317676      0.416782         0.339747        0.096444   \n",
       "8        3.829948      0.295035         0.357485        0.121151   \n",
       "9        6.351973      0.540787         0.470245        0.143196   \n",
       "10       7.665792      0.610991         0.496756        0.164851   \n",
       "11       6.020743      0.076090         0.406754        0.138311   \n",
       "12       3.570344      1.114939         0.211549        0.020979   \n",
       "13       3.086701      0.152967         0.207146        0.039035   \n",
       "14       2.239944      0.178535         0.215869        0.016030   \n",
       "15       3.120496      0.338670         0.231616        0.033727   \n",
       "16       3.340493      0.160609         0.280253        0.102356   \n",
       "17       2.397735      0.284636         0.286100        0.114380   \n",
       "18       3.915668      0.441586         0.232191        0.068015   \n",
       "19       4.797312      0.149895         0.320819        0.176451   \n",
       "20       3.257850      0.177980         0.324824        0.059822   \n",
       "21       6.746908      0.868581         0.482052        0.173311   \n",
       "22       7.537841      0.589888         0.463529        0.112301   \n",
       "23       6.238195      0.765762         0.441028        0.154699   \n",
       "24       2.324199      0.435338         0.188184        0.032121   \n",
       "25       2.214063      0.057130         0.186553        0.036686   \n",
       "26       1.799853      0.257190         0.205163        0.051080   \n",
       "27       2.277235      0.112759         0.205107        0.004444   \n",
       "28       2.668551      0.086737         0.232035        0.055312   \n",
       "29       1.978885      0.120054         0.232963        0.078864   \n",
       "30       2.789785      0.280597         0.260028        0.085120   \n",
       "31       3.483399      0.281094         0.284199        0.073868   \n",
       "32       2.483238      0.172223         0.237828        0.097553   \n",
       "33       4.680945      0.566995         0.528591        0.297240   \n",
       "34       5.733126      0.306495         0.469986        0.379053   \n",
       "35       3.948307      0.397142         0.406174        0.174878   \n",
       "36       2.662696      0.937792         0.199899        0.041986   \n",
       "37       2.236734      0.091245         0.189377        0.031969   \n",
       "38       1.628574      0.082701         0.195515        0.030234   \n",
       "39       2.174192      0.196103         0.243445        0.084013   \n",
       "40       2.706820      0.130739         0.222837        0.072213   \n",
       "41       2.228038      0.323113         0.255989        0.045270   \n",
       "42       2.685289      0.222952         0.255982        0.090630   \n",
       "43       3.249573      0.116436         0.273327        0.116947   \n",
       "44       2.501010      0.166296         0.379093        0.212618   \n",
       "45       4.848362      0.569429         0.416948        0.151584   \n",
       "46       5.712243      0.369797         0.467753        0.177920   \n",
       "47       5.250894      0.778133         0.520356        0.383205   \n",
       "\n",
       "   param_batch_size param_epochs param_init_mode param_neurons  \\\n",
       "0               128            5       he_normal            32   \n",
       "1               128            5       he_normal            32   \n",
       "2               128            5       he_normal            32   \n",
       "3               128            5       he_normal            64   \n",
       "4               128            5       he_normal            64   \n",
       "5               128            5       he_normal            64   \n",
       "6               128            5       he_normal           128   \n",
       "7               128            5       he_normal           128   \n",
       "8               128            5       he_normal           128   \n",
       "9               128            5       he_normal           256   \n",
       "10              128            5       he_normal           256   \n",
       "11              128            5       he_normal           256   \n",
       "12              128            5      he_uniform            32   \n",
       "13              128            5      he_uniform            32   \n",
       "14              128            5      he_uniform            32   \n",
       "15              128            5      he_uniform            64   \n",
       "16              128            5      he_uniform            64   \n",
       "17              128            5      he_uniform            64   \n",
       "18              128            5      he_uniform           128   \n",
       "19              128            5      he_uniform           128   \n",
       "20              128            5      he_uniform           128   \n",
       "21              128            5      he_uniform           256   \n",
       "22              128            5      he_uniform           256   \n",
       "23              128            5      he_uniform           256   \n",
       "24              264            5       he_normal            32   \n",
       "25              264            5       he_normal            32   \n",
       "26              264            5       he_normal            32   \n",
       "27              264            5       he_normal            64   \n",
       "28              264            5       he_normal            64   \n",
       "29              264            5       he_normal            64   \n",
       "30              264            5       he_normal           128   \n",
       "31              264            5       he_normal           128   \n",
       "32              264            5       he_normal           128   \n",
       "33              264            5       he_normal           256   \n",
       "34              264            5       he_normal           256   \n",
       "35              264            5       he_normal           256   \n",
       "36              264            5      he_uniform            32   \n",
       "37              264            5      he_uniform            32   \n",
       "38              264            5      he_uniform            32   \n",
       "39              264            5      he_uniform            64   \n",
       "40              264            5      he_uniform            64   \n",
       "41              264            5      he_uniform            64   \n",
       "42              264            5      he_uniform           128   \n",
       "43              264            5      he_uniform           128   \n",
       "44              264            5      he_uniform           128   \n",
       "45              264            5      he_uniform           256   \n",
       "46              264            5      he_uniform           256   \n",
       "47              264            5      he_uniform           256   \n",
       "\n",
       "   param_optimizer                                             params  \\\n",
       "0             Adam  {'batch_size': 128, 'epochs': 5, 'init_mode': ...   \n",
       "1            Nadam  {'batch_size': 128, 'epochs': 5, 'init_mode': ...   \n",
       "2          Adagrad  {'batch_size': 128, 'epochs': 5, 'init_mode': ...   \n",
       "3             Adam  {'batch_size': 128, 'epochs': 5, 'init_mode': ...   \n",
       "4            Nadam  {'batch_size': 128, 'epochs': 5, 'init_mode': ...   \n",
       "5          Adagrad  {'batch_size': 128, 'epochs': 5, 'init_mode': ...   \n",
       "6             Adam  {'batch_size': 128, 'epochs': 5, 'init_mode': ...   \n",
       "7            Nadam  {'batch_size': 128, 'epochs': 5, 'init_mode': ...   \n",
       "8          Adagrad  {'batch_size': 128, 'epochs': 5, 'init_mode': ...   \n",
       "9             Adam  {'batch_size': 128, 'epochs': 5, 'init_mode': ...   \n",
       "10           Nadam  {'batch_size': 128, 'epochs': 5, 'init_mode': ...   \n",
       "11         Adagrad  {'batch_size': 128, 'epochs': 5, 'init_mode': ...   \n",
       "12            Adam  {'batch_size': 128, 'epochs': 5, 'init_mode': ...   \n",
       "13           Nadam  {'batch_size': 128, 'epochs': 5, 'init_mode': ...   \n",
       "14         Adagrad  {'batch_size': 128, 'epochs': 5, 'init_mode': ...   \n",
       "15            Adam  {'batch_size': 128, 'epochs': 5, 'init_mode': ...   \n",
       "16           Nadam  {'batch_size': 128, 'epochs': 5, 'init_mode': ...   \n",
       "17         Adagrad  {'batch_size': 128, 'epochs': 5, 'init_mode': ...   \n",
       "18            Adam  {'batch_size': 128, 'epochs': 5, 'init_mode': ...   \n",
       "19           Nadam  {'batch_size': 128, 'epochs': 5, 'init_mode': ...   \n",
       "20         Adagrad  {'batch_size': 128, 'epochs': 5, 'init_mode': ...   \n",
       "21            Adam  {'batch_size': 128, 'epochs': 5, 'init_mode': ...   \n",
       "22           Nadam  {'batch_size': 128, 'epochs': 5, 'init_mode': ...   \n",
       "23         Adagrad  {'batch_size': 128, 'epochs': 5, 'init_mode': ...   \n",
       "24            Adam  {'batch_size': 264, 'epochs': 5, 'init_mode': ...   \n",
       "25           Nadam  {'batch_size': 264, 'epochs': 5, 'init_mode': ...   \n",
       "26         Adagrad  {'batch_size': 264, 'epochs': 5, 'init_mode': ...   \n",
       "27            Adam  {'batch_size': 264, 'epochs': 5, 'init_mode': ...   \n",
       "28           Nadam  {'batch_size': 264, 'epochs': 5, 'init_mode': ...   \n",
       "29         Adagrad  {'batch_size': 264, 'epochs': 5, 'init_mode': ...   \n",
       "30            Adam  {'batch_size': 264, 'epochs': 5, 'init_mode': ...   \n",
       "31           Nadam  {'batch_size': 264, 'epochs': 5, 'init_mode': ...   \n",
       "32         Adagrad  {'batch_size': 264, 'epochs': 5, 'init_mode': ...   \n",
       "33            Adam  {'batch_size': 264, 'epochs': 5, 'init_mode': ...   \n",
       "34           Nadam  {'batch_size': 264, 'epochs': 5, 'init_mode': ...   \n",
       "35         Adagrad  {'batch_size': 264, 'epochs': 5, 'init_mode': ...   \n",
       "36            Adam  {'batch_size': 264, 'epochs': 5, 'init_mode': ...   \n",
       "37           Nadam  {'batch_size': 264, 'epochs': 5, 'init_mode': ...   \n",
       "38         Adagrad  {'batch_size': 264, 'epochs': 5, 'init_mode': ...   \n",
       "39            Adam  {'batch_size': 264, 'epochs': 5, 'init_mode': ...   \n",
       "40           Nadam  {'batch_size': 264, 'epochs': 5, 'init_mode': ...   \n",
       "41         Adagrad  {'batch_size': 264, 'epochs': 5, 'init_mode': ...   \n",
       "42            Adam  {'batch_size': 264, 'epochs': 5, 'init_mode': ...   \n",
       "43           Nadam  {'batch_size': 264, 'epochs': 5, 'init_mode': ...   \n",
       "44         Adagrad  {'batch_size': 264, 'epochs': 5, 'init_mode': ...   \n",
       "45            Adam  {'batch_size': 264, 'epochs': 5, 'init_mode': ...   \n",
       "46           Nadam  {'batch_size': 264, 'epochs': 5, 'init_mode': ...   \n",
       "47         Adagrad  {'batch_size': 264, 'epochs': 5, 'init_mode': ...   \n",
       "\n",
       "    split0_test_score  split1_test_score  split2_test_score  \\\n",
       "0            0.700270           0.679790           0.758057   \n",
       "1            0.779171           0.787109           0.741705   \n",
       "2            0.628195           0.598190           0.632640   \n",
       "3            0.702651           0.761867           0.749960   \n",
       "4            0.789967           0.800286           0.837276   \n",
       "5            0.753453           0.700270           0.729798   \n",
       "6            0.722496           0.848071           0.736149   \n",
       "7            0.841403           0.800445           0.751230   \n",
       "8            0.722019           0.739165           0.772345   \n",
       "9            0.774726           0.777743           0.852040   \n",
       "10           0.782981           0.714399           0.673758   \n",
       "11           0.774885           0.738212           0.767741   \n",
       "12           0.745039           0.670424           0.713605   \n",
       "13           0.822670           0.768852           0.822670   \n",
       "14           0.639149           0.629306           0.623750   \n",
       "15           0.795523           0.738054           0.713764   \n",
       "16           0.807430           0.738847           0.707573   \n",
       "17           0.668519           0.686776           0.701699   \n",
       "18           0.773297           0.757422           0.817114   \n",
       "19           0.793301           0.801238           0.827909   \n",
       "20           0.753135           0.614224           0.751072   \n",
       "21           0.822988           0.785204           0.763137   \n",
       "22           0.645023           0.739324           0.733132   \n",
       "23           0.695348           0.805048           0.773139   \n",
       "24           0.632481           0.607557           0.562311   \n",
       "25           0.659311           0.729005           0.726147   \n",
       "26           0.596920           0.604858           0.517542   \n",
       "27           0.691697           0.723131           0.694396   \n",
       "28           0.792507           0.741070           0.705826   \n",
       "29           0.730751           0.689316           0.645658   \n",
       "30           0.752500           0.743293           0.722178   \n",
       "31           0.776314           0.548024           0.742499   \n",
       "32           0.624861           0.754564           0.703445   \n",
       "33           0.749484           0.699476           0.770281   \n",
       "34           0.703286           0.794729           0.735990   \n",
       "35           0.735831           0.699159           0.646293   \n",
       "36           0.628036           0.542943           0.505001   \n",
       "37           0.668678           0.752659           0.725512   \n",
       "38           0.607557           0.632958           0.546912   \n",
       "39           0.666296           0.751548           0.649786   \n",
       "40           0.665185           0.779013           0.742816   \n",
       "41           0.693920           0.589935           0.671218   \n",
       "42           0.733608           0.695983           0.773297   \n",
       "43           0.631529           0.722813           0.701540   \n",
       "44           0.702810           0.573901           0.668360   \n",
       "45           0.766788           0.763931           0.767423   \n",
       "46           0.670424           0.754247           0.628036   \n",
       "47           0.691221           0.688204           0.700587   \n",
       "\n",
       "    split3_test_score  split4_test_score  mean_test_score  std_test_score  \\\n",
       "0            0.630676           0.689584         0.691676        0.040858   \n",
       "1            0.790092           0.748809         0.769377        0.020141   \n",
       "2            0.695300           0.583042         0.627474        0.038625   \n",
       "3            0.787234           0.753255         0.750994        0.027476   \n",
       "4            0.766751           0.787710         0.796398        0.023160   \n",
       "5            0.741188           0.725786         0.730099        0.017759   \n",
       "6            0.741505           0.825341         0.774712        0.051500   \n",
       "7            0.848841           0.729120         0.794208        0.047612   \n",
       "8            0.783582           0.725945         0.748611        0.024885   \n",
       "9            0.723563           0.763417         0.778298        0.041637   \n",
       "10           0.795967           0.690378         0.731497        0.049242   \n",
       "11           0.768498           0.824547         0.774777        0.027939   \n",
       "12           0.707844           0.637663         0.694915        0.037166   \n",
       "13           0.806288           0.791204         0.802337        0.020434   \n",
       "14           0.673706           0.603525         0.633887        0.023060   \n",
       "15           0.807082           0.815815         0.774047        0.040563   \n",
       "16           0.772467           0.791045         0.763472        0.036068   \n",
       "17           0.766116           0.737059         0.712034        0.035184   \n",
       "18           0.796443           0.776913         0.784238        0.020602   \n",
       "19           0.721340           0.768974         0.782552        0.035926   \n",
       "20           0.678628           0.738965         0.707205        0.053860   \n",
       "21           0.751191           0.786758         0.781855        0.024568   \n",
       "22           0.655923           0.835345         0.721749        0.068647   \n",
       "23           0.639886           0.742617         0.731208        0.058252   \n",
       "24           0.554621           0.611464         0.593687        0.030080   \n",
       "25           0.659892           0.673230         0.689517        0.031484   \n",
       "26           0.570181           0.574468         0.572794        0.030570   \n",
       "27           0.690219           0.691331         0.698155        0.012563   \n",
       "28           0.616704           0.781677         0.727557        0.063389   \n",
       "29           0.686726           0.642585         0.679007        0.032502   \n",
       "30           0.745316           0.737377         0.740133        0.010196   \n",
       "31           0.802477           0.745792         0.723021        0.090195   \n",
       "32           0.710384           0.635440         0.685739        0.048773   \n",
       "33           0.759130           0.730391         0.741752        0.024868   \n",
       "34           0.643220           0.752461         0.725937        0.050778   \n",
       "35           0.702922           0.747380         0.706317        0.035279   \n",
       "36           0.651318           0.645602         0.594580        0.059384   \n",
       "37           0.674659           0.694983         0.703298        0.031679   \n",
       "38           0.608447           0.624484         0.604072        0.030165   \n",
       "39           0.683709           0.698158         0.689899        0.034852   \n",
       "40           0.666720           0.646713         0.700089        0.051450   \n",
       "41           0.538901           0.655129         0.629820        0.057132   \n",
       "42           0.713560           0.755478         0.734385        0.027814   \n",
       "43           0.584789           0.734836         0.675101        0.057617   \n",
       "44           0.608606           0.661162         0.642968        0.045843   \n",
       "45           0.785011           0.740076         0.764646        0.014361   \n",
       "46           0.687837           0.596539         0.667417        0.053926   \n",
       "47           0.608765           0.615751         0.660906        0.039991   \n",
       "\n",
       "    rank_test_score  \n",
       "0                33  \n",
       "1                11  \n",
       "2                44  \n",
       "3                14  \n",
       "4                 2  \n",
       "5                21  \n",
       "6                 9  \n",
       "7                 3  \n",
       "8                15  \n",
       "9                 7  \n",
       "10               19  \n",
       "11                8  \n",
       "12               32  \n",
       "13                1  \n",
       "14               42  \n",
       "15               10  \n",
       "16               13  \n",
       "17               26  \n",
       "18                4  \n",
       "19                5  \n",
       "20               27  \n",
       "21                6  \n",
       "22               25  \n",
       "23               20  \n",
       "24               47  \n",
       "25               35  \n",
       "26               48  \n",
       "27               31  \n",
       "28               22  \n",
       "29               37  \n",
       "30               17  \n",
       "31               24  \n",
       "32               36  \n",
       "33               16  \n",
       "34               23  \n",
       "35               28  \n",
       "36               46  \n",
       "37               29  \n",
       "38               45  \n",
       "39               34  \n",
       "40               30  \n",
       "41               43  \n",
       "42               18  \n",
       "43               38  \n",
       "44               41  \n",
       "45               12  \n",
       "46               39  \n",
       "47               40  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot learning curves\n",
    "pyplot.title('accuracy')\n",
    "pyplot.xlabel('hyperparameter')\n",
    "pyplot.ylabel('Score')\n",
    "pyplot.scatter(grid_result.cv_results_['rank_test_score'],grid_result.cv_results_['mean_test_score'] )\n",
    "pyplot.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
